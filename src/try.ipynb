{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree as tr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data_path = \"../data/cesd_total.csv\"\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-1].values\n",
    "y = data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# let's further split the training set into a training and a validation set (15%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                  y_train,\n",
    "                                                  test_size=X_test.shape[0] / X_train.shape[0],  \n",
    "                                                  random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def identify_variable_types(data):\n",
    "    continuous_vars = []\n",
    "    dummy_vars = []\n",
    "\n",
    "    num_columns = data.shape[1]\n",
    "    \n",
    "    for i in range(num_columns):\n",
    "        unique_values = np.unique(data[:, i])\n",
    "        if len(unique_values) == 2 and np.array_equal(unique_values, [0, 1]):\n",
    "            dummy_vars.append(i)\n",
    "        else:\n",
    "            continuous_vars.append(i)\n",
    "    \n",
    "    return continuous_vars, dummy_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "continuous_cols_train, dummy_cols_train = identify_variable_types(X_train)\n",
    "\n",
    "# Define the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), continuous_cols_train),\n",
    "        ('dummy', 'passthrough', dummy_cols_train)  # Leave dummy variables unchanged or use StandardScaler() if needed\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "\n",
    "continuous_cols_val, dummy_cols_val = identify_variable_types(X_val)\n",
    "\n",
    "# Define the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), continuous_cols_val),\n",
    "        ('dummy', 'passthrough', dummy_cols_val)  # Leave dummy variables unchanged or use StandardScaler() if needed\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "X_val = preprocessor.fit_transform(X_val)\n",
    "\n",
    "continuous_cols_test, dummy_cols_test = identify_variable_types(X_test)\n",
    "\n",
    "# Define the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), continuous_cols_test),\n",
    "        ('dummy', 'passthrough', dummy_cols_test)  # Leave dummy variables unchanged or use StandardScaler() if needed\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "X_test = preprocessor.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_performances = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def run_on_splits(func):\n",
    "    def _run_loop(*args, **kwargs):\n",
    "        for x,y,nsplit in zip([X_train, X_val, X_test],\n",
    "                              [y_train, y_val, y_test],\n",
    "                              ['train', 'val', 'test']):\n",
    "            func(*args, X=x, y=y, nsplit=nsplit, **kwargs)\n",
    "    return _run_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "@run_on_splits\n",
    "def evaluate(model, X, y, nsplit, model_name, constant_value=None):\n",
    "    ''' Evaluates the performance of a model \n",
    "    Args:\n",
    "        model (sklearn.Estimator): fitted sklearn estimator\n",
    "        X (np.array): predictors\n",
    "        y (np.array): true outcome\n",
    "        nsplit (str): name of the split\n",
    "        model_name (str): string id of the model\n",
    "        constant_value (int or None): relevant if the model predicts a constant\n",
    "    '''\n",
    "    if constant_value is not None:\n",
    "        preds = np.array([constant_value] * y.shape[0])\n",
    "    else:\n",
    "        preds = model.predict(X)\n",
    "    r2 = r2_score(y, preds)\n",
    "    performance = np.sqrt(mean_squared_error(y, preds))\n",
    "    model_performances.append({'model': model_name,\n",
    "                         'split': nsplit,\n",
    "                         'rmse': round(performance, 4),\n",
    "                         'r2': round(r2, 4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_performances = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "evaluate(model=None, model_name='dummy', constant_value=y_train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def run_dummy(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    evaluate(model=None, model_name='dummy', constant_value=y_train.mean())\n",
    "    print(model_performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'dummy', 'split': 'train', 'rmse': 11.2661, 'r2': 0.0}, {'model': 'dummy', 'split': 'val', 'rmse': 12.044, 'r2': -0.0241}, {'model': 'dummy', 'split': 'test', 'rmse': 12.5069, 'r2': -0.0051}, {'model': 'dummy', 'split': 'train', 'rmse': 11.2661, 'r2': 0.0}, {'model': 'dummy', 'split': 'val', 'rmse': 12.044, 'r2': -0.0241}, {'model': 'dummy', 'split': 'test', 'rmse': 12.5069, 'r2': -0.0051}]\n"
     ]
    }
   ],
   "source": [
    "run_dummy(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "evaluate(model=reg, model_name='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "models = {} # storing fitted models in the next chunk\n",
    "models['linear-0.0'] = reg\n",
    "\n",
    "for alpha in [0.01, 0.1, 0.2, 0.5, 1.0, 20.0, 10.0, 100.0, 1000.0]:\n",
    "    for est in [Lasso, Ridge]:\n",
    "        if est == Lasso:\n",
    "            id = 'lasso'\n",
    "        else:\n",
    "            id = 'ridge'\n",
    "        reg = est(alpha=alpha).fit(X_train, y_train)\n",
    "        models[f'{id}-{alpha}'] = reg\n",
    "        evaluate(model=reg, model_name=f'{id}-alpha-{alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def _compute_bias(pred, true):\n",
    "    ''' Function to compute bias. \n",
    "        Note that here we compute the average squared bias of the model\n",
    "        over all data points to get a sense for its tendency to make \n",
    "        systematically \"off-target\" predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (np.array): array of shape (n_samples, n_bootstrap), where n_samples is the size\n",
    "            of the test set, and n_bootstraps is how many times we sample data from the training\n",
    "            set and fit our model\n",
    "        true (np.array): predictions of the true model. It is an array of shape (n_samples)\n",
    "    '''\n",
    "    mpred = pred.mean(axis=1)\n",
    "    return (((true - mpred)**2)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def _compute_variance(pred):\n",
    "    ''' Function to compute variance\n",
    "    Args:\n",
    "        pred (np.array): array of shape (n_samples, n_bootstrap), where n_samples is the size\n",
    "            of the test set, and n_bootstraps is how many times we sample data from the training\n",
    "            set and fit our model\n",
    "    '''\n",
    "    return pred.var(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "max_degree = 10 # this defines the most complex model we will fit (with x, ..., x^10) as inputs\n",
    "n_sim = 1000 # this defines how many times we do bootstrapping\n",
    "performances = [] # here we will store performances\n",
    "results = [] # here we store performances as a DF (see logic in the loop)\n",
    "outputs = ['mtype', # this is the information we want to store in the DF\n",
    "           'complexity', \n",
    "           'bias^2', \n",
    "           'variance', \n",
    "           'mse']\n",
    "\n",
    "for c in range(1, max_degree):\n",
    "    reg = LinearRegression() # initialize the model \n",
    "    pred_all = np.zeros(shape=(y_test.shape[0], n_sim)) # empty array, which will store our predictions across bootstraps\n",
    "    for sim in range(n_sim): # bootstrap n_sim times\n",
    "        x, _, y, _ = train_test_split(X_train, y_train, train_size=.5) # sample half of the data\n",
    "        transformer = PolynomialFeatures(degree=c)\n",
    "        x_fit = transformer.fit_transform(x[:,1].reshape(-1,1)) # take x, and add all polynomials all the way to c (current complexity)\n",
    "        x_test = transformer.fit_transform(X_test[:,1].reshape(-1,1)) # do the same with the test set\n",
    "        reg.fit(x_fit,y) # fit the model\n",
    "        preds = reg.predict(x_test) # predict the outcomes\n",
    "        pred_all[:,sim] = preds # store that in the big result matrix\n",
    "\n",
    "    # once we are done bootstrapping, we have predictions for all models\n",
    "    mse = np.mean([np.sqrt(mean_squared_error(y_test, \n",
    "                                              pred_all[:,sim])) # compute the average MSE of the models\n",
    "                    for sim in range(n_sim)])\n",
    "    bias = _compute_bias(pred_all, y_test) # get bias\n",
    "    variance = _compute_variance(pred_all) # get variance\n",
    "    info = ('linear', x_fit.shape[1]-1, bias, variance, mse)\n",
    "    performances.append(info) # append model info to performances\n",
    "\n",
    "# Getting the results into a dataframe \n",
    "result = pd.DataFrame(performances, columns=outputs)\n",
    "for c in ['bias^2', 'variance', 'mse']:\n",
    "    result[f'{c}-scaled'] = MinMaxScaler().fit_transform(result[c].values.reshape(-1,1)) # rescale, so bias and variance are on the same scale\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[    mtype  complexity      bias^2     variance        mse  bias^2-scaled  \\\n",
       " 0  linear           1  159.392657     0.355622  12.638962       0.000000   \n",
       " 1  linear           2  159.544480     0.475238  12.649676       0.000842   \n",
       " 2  linear           3  159.761346     0.695780  12.666909       0.002044   \n",
       " 3  linear           4  160.288656     1.092431  12.703008       0.004968   \n",
       " 4  linear           5  160.013227     1.941988  12.723786       0.003441   \n",
       " 5  linear           6  160.332740     6.413099  12.872980       0.005212   \n",
       " 6  linear           7  160.160580    36.989057  13.362098       0.004258   \n",
       " 7  linear           8  170.943303   423.518532  15.186416       0.064041   \n",
       " 8  linear           9  339.754791  7660.842021  27.102557       1.000000   \n",
       " \n",
       "    variance-scaled  mse-scaled  \n",
       " 0         0.000000    0.000000  \n",
       " 1         0.000016    0.000741  \n",
       " 2         0.000044    0.001932  \n",
       " 3         0.000096    0.004428  \n",
       " 4         0.000207    0.005865  \n",
       " 5         0.000791    0.016180  \n",
       " 6         0.004782    0.049997  \n",
       " 7         0.055240    0.176129  \n",
       " 8         1.000000    1.000000  ]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'dummy', 'split': 'train', 'rmse': 11.2661, 'r2': 0.0},\n",
       " {'model': 'dummy', 'split': 'val', 'rmse': 12.044, 'r2': -0.0241},\n",
       " {'model': 'dummy', 'split': 'test', 'rmse': 12.5069, 'r2': -0.0051},\n",
       " {'model': 'dummy', 'split': 'train', 'rmse': 11.2661, 'r2': 0.0},\n",
       " {'model': 'dummy', 'split': 'val', 'rmse': 12.044, 'r2': -0.0241},\n",
       " {'model': 'dummy', 'split': 'test', 'rmse': 12.5069, 'r2': -0.0051},\n",
       " {'model': 'linear', 'split': 'train', 'rmse': 6.9475, 'r2': 0.6197},\n",
       " {'model': 'linear',\n",
       "  'split': 'val',\n",
       "  'rmse': 55978618892232.44,\n",
       "  'r2': -2.2122366196762484e+25},\n",
       " {'model': 'linear',\n",
       "  'split': 'test',\n",
       "  'rmse': 45297602434711.734,\n",
       "  'r2': -1.3183983522923074e+25},\n",
       " {'model': 'lasso-alpha-0.01', 'split': 'train', 'rmse': 7.1045, 'r2': 0.6023},\n",
       " {'model': 'lasso-alpha-0.01', 'split': 'val', 'rmse': 12.3959, 'r2': -0.0848},\n",
       " {'model': 'lasso-alpha-0.01', 'split': 'test', 'rmse': 11.7864, 'r2': 0.1074},\n",
       " {'model': 'ridge-alpha-0.01', 'split': 'train', 'rmse': 6.9584, 'r2': 0.6185},\n",
       " {'model': 'ridge-alpha-0.01',\n",
       "  'split': 'val',\n",
       "  'rmse': 57.0004,\n",
       "  'r2': -21.9374},\n",
       " {'model': 'ridge-alpha-0.01',\n",
       "  'split': 'test',\n",
       "  'rmse': 44.7042,\n",
       "  'r2': -11.8408},\n",
       " {'model': 'lasso-alpha-0.1', 'split': 'train', 'rmse': 7.5351, 'r2': 0.5527},\n",
       " {'model': 'lasso-alpha-0.1', 'split': 'val', 'rmse': 11.616, 'r2': 0.0474},\n",
       " {'model': 'lasso-alpha-0.1', 'split': 'test', 'rmse': 10.4292, 'r2': 0.3011},\n",
       " {'model': 'ridge-alpha-0.1', 'split': 'train', 'rmse': 6.9678, 'r2': 0.6175},\n",
       " {'model': 'ridge-alpha-0.1', 'split': 'val', 'rmse': 42.8675, 'r2': -11.9731},\n",
       " {'model': 'ridge-alpha-0.1', 'split': 'test', 'rmse': 32.13, 'r2': -5.6331},\n",
       " {'model': 'lasso-alpha-0.2', 'split': 'train', 'rmse': 7.765, 'r2': 0.525},\n",
       " {'model': 'lasso-alpha-0.2', 'split': 'val', 'rmse': 11.4901, 'r2': 0.068},\n",
       " {'model': 'lasso-alpha-0.2', 'split': 'test', 'rmse': 10.2731, 'r2': 0.3219},\n",
       " {'model': 'ridge-alpha-0.2', 'split': 'train', 'rmse': 6.9799, 'r2': 0.6162},\n",
       " {'model': 'ridge-alpha-0.2', 'split': 'val', 'rmse': 34.0786, 'r2': -7.1988},\n",
       " {'model': 'ridge-alpha-0.2', 'split': 'test', 'rmse': 25.9619, 'r2': -3.3308},\n",
       " {'model': 'lasso-alpha-0.5', 'split': 'train', 'rmse': 8.0865, 'r2': 0.4848},\n",
       " {'model': 'lasso-alpha-0.5', 'split': 'val', 'rmse': 11.0031, 'r2': 0.1453},\n",
       " {'model': 'lasso-alpha-0.5', 'split': 'test', 'rmse': 10.1461, 'r2': 0.3386},\n",
       " {'model': 'ridge-alpha-0.5', 'split': 'train', 'rmse': 7.0072, 'r2': 0.6132},\n",
       " {'model': 'ridge-alpha-0.5', 'split': 'val', 'rmse': 23.3348, 'r2': -2.8441},\n",
       " {'model': 'ridge-alpha-0.5', 'split': 'test', 'rmse': 18.3656, 'r2': -1.1672},\n",
       " {'model': 'lasso-alpha-1.0', 'split': 'train', 'rmse': 8.2878, 'r2': 0.4588},\n",
       " {'model': 'lasso-alpha-1.0', 'split': 'val', 'rmse': 11.0569, 'r2': 0.1369},\n",
       " {'model': 'lasso-alpha-1.0', 'split': 'test', 'rmse': 10.1979, 'r2': 0.3318},\n",
       " {'model': 'ridge-alpha-1.0', 'split': 'train', 'rmse': 7.0342, 'r2': 0.6102},\n",
       " {'model': 'ridge-alpha-1.0', 'split': 'val', 'rmse': 17.7418, 'r2': -1.2222},\n",
       " {'model': 'ridge-alpha-1.0', 'split': 'test', 'rmse': 14.4503, 'r2': -0.3417},\n",
       " {'model': 'lasso-alpha-20.0', 'split': 'train', 'rmse': 11.2661, 'r2': 0.0},\n",
       " {'model': 'lasso-alpha-20.0', 'split': 'val', 'rmse': 12.044, 'r2': -0.0241},\n",
       " {'model': 'lasso-alpha-20.0',\n",
       "  'split': 'test',\n",
       "  'rmse': 12.5069,\n",
       "  'r2': -0.0051},\n",
       " {'model': 'ridge-alpha-20.0', 'split': 'train', 'rmse': 7.2284, 'r2': 0.5883},\n",
       " {'model': 'ridge-alpha-20.0', 'split': 'val', 'rmse': 11.9035, 'r2': -0.0003},\n",
       " {'model': 'ridge-alpha-20.0', 'split': 'test', 'rmse': 10.63, 'r2': 0.274},\n",
       " {'model': 'lasso-alpha-10.0', 'split': 'train', 'rmse': 11.2661, 'r2': 0.0},\n",
       " {'model': 'lasso-alpha-10.0', 'split': 'val', 'rmse': 12.044, 'r2': -0.0241},\n",
       " {'model': 'lasso-alpha-10.0',\n",
       "  'split': 'test',\n",
       "  'rmse': 12.5069,\n",
       "  'r2': -0.0051},\n",
       " {'model': 'ridge-alpha-10.0', 'split': 'train', 'rmse': 7.1671, 'r2': 0.5953},\n",
       " {'model': 'ridge-alpha-10.0', 'split': 'val', 'rmse': 12.2179, 'r2': -0.0538},\n",
       " {'model': 'ridge-alpha-10.0', 'split': 'test', 'rmse': 10.8991, 'r2': 0.2367},\n",
       " {'model': 'lasso-alpha-100.0', 'split': 'train', 'rmse': 11.2661, 'r2': 0.0},\n",
       " {'model': 'lasso-alpha-100.0', 'split': 'val', 'rmse': 12.044, 'r2': -0.0241},\n",
       " {'model': 'lasso-alpha-100.0',\n",
       "  'split': 'test',\n",
       "  'rmse': 12.5069,\n",
       "  'r2': -0.0051},\n",
       " {'model': 'ridge-alpha-100.0', 'split': 'train', 'rmse': 7.4308, 'r2': 0.565},\n",
       " {'model': 'ridge-alpha-100.0', 'split': 'val', 'rmse': 11.3323, 'r2': 0.0934},\n",
       " {'model': 'ridge-alpha-100.0',\n",
       "  'split': 'test',\n",
       "  'rmse': 10.0807,\n",
       "  'r2': 0.3471},\n",
       " {'model': 'lasso-alpha-1000.0', 'split': 'train', 'rmse': 11.2661, 'r2': 0.0},\n",
       " {'model': 'lasso-alpha-1000.0',\n",
       "  'split': 'val',\n",
       "  'rmse': 12.044,\n",
       "  'r2': -0.0241},\n",
       " {'model': 'lasso-alpha-1000.0',\n",
       "  'split': 'test',\n",
       "  'rmse': 12.5069,\n",
       "  'r2': -0.0051},\n",
       " {'model': 'ridge-alpha-1000.0',\n",
       "  'split': 'train',\n",
       "  'rmse': 8.114,\n",
       "  'r2': 0.4813},\n",
       " {'model': 'ridge-alpha-1000.0',\n",
       "  'split': 'val',\n",
       "  'rmse': 10.6286,\n",
       "  'r2': 0.2025},\n",
       " {'model': 'ridge-alpha-1000.0',\n",
       "  'split': 'test',\n",
       "  'rmse': 9.7635,\n",
       "  'r2': 0.3875}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
